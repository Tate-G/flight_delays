{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Delay Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains and saves the deployment model to be used by the Flight Delay Predictor. Trains a neural network model based on years of flight data from the US Bureau of Transportation Statistics. Predicts flight delays based on the flight schedule, the airline, and the airports. A python notebook showing model cross-validation as well as a summary report of the modeling procedure and general findings are available at https://github.com/Tate-G/portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Number of GPUs Available:  1\n",
      "Tensorflow version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print('Number of GPUs Available: ', len(tf.config.list_physical_devices('GPU')))\n",
    "print('Tensorflow version: '+tf.version.VERSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input variables for this run of modeling and evaluation. User can train a new model or load a previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output file names\n",
    "output_folder='7-22_May2019-Apr2021/'\n",
    "\n",
    "\n",
    "#define the first and last month of the training data\n",
    "train_list_start='2019_5'\n",
    "train_list_end='2021_4'\n",
    "#define the first and last month of the validation data\n",
    "val_list_start='2021_5'\n",
    "val_list_end='2021_5'\n",
    "\n",
    "\n",
    "#define predictor variables\n",
    "X_vars=['Month','DayOfWeek','Reporting_Airline',\n",
    "            'Origin','Dest','OriginState','DestState',\n",
    "            'CRSDepTime','CRSArrTime',\n",
    "            'CRSElapsedTime']\n",
    "X_vars_categorical=['Reporting_Airline','Origin','Dest','OriginState','DestState']\n",
    "X_vars_hours=['CRSDepTime','CRSArrTime']\n",
    "X_vars_cyclical_dict={'Month':12,'DayOfWeek':7,'CRSDepTime':24,'CRSArrTime':24}\n",
    "X_vars_normalize=['CRSElapsedTime']\n",
    "X_vars_log=['CRSElapsedTime'] #variables to log while normalizing, must be a subset of X_vars_normalize\n",
    "y_var=['ArrDelayMinutes']\n",
    "\n",
    "#define number of busiest airports to consider\n",
    "num_airports=100\n",
    "\n",
    "#define the delay length to consider\n",
    "delay_minutes=15\n",
    "\n",
    "#define the random fraction of training data to use\n",
    "#true random subset if imbalanced==False, subset with increased % minority class (delays) if imbalanced==True\n",
    "imbalanced=False\n",
    "rand_frac_train=0.35\n",
    "rand_frac_val=1\n",
    "\n",
    "\n",
    "#define modeling parameters\n",
    "Epochs = 15\n",
    "Batch_Size = 2048\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Supporting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to support reading input data from csv files. Files stored in subfolder 'US_DOT/On-Time/'. Dataset csv files available for download at https://www.transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to import csv files into dataframe\n",
    "def read_flight_csv(filenames,col_names):\n",
    "    df=pd.read_csv(filenames[0],usecols=col_names)\n",
    "    print('Read '+filenames[0])\n",
    "    if len(filenames)>1:\n",
    "        for i in np.arange(1,len(filenames)):\n",
    "            df=df.append(pd.read_csv(filenames[i],usecols=col_names))\n",
    "            print('Read '+filenames[i])\n",
    "    df=df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "#make list of filenames to import in a range of months starting from given YYYY_MM strings\n",
    "def make_filenames(start_date,end_date):\n",
    "    filename_start='US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_'\n",
    "    filename_ext='.csv'\n",
    "    start=datetime.strptime(start_date,'%Y_%m')\n",
    "    end=datetime.strptime(end_date,'%Y_%m')\n",
    "    dates=pd.date_range(start,end,freq='MS')\n",
    "    date_list=dates.strftime('%Y_%#m').tolist()\n",
    "    files=[filename_start+date+filename_ext for date in date_list]\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to support restricting data to only include flights to and from a given number of the busiest airports, and to clean airline names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the given number of busiest airports in a dataset\n",
    "def airport_select(df,num_airports):\n",
    "    arrivals=df.groupby(['Dest']).size()\n",
    "    departures=df.groupby(['Origin']).size()\n",
    "    flights=arrivals+departures\n",
    "    airports=flights.sort_values(ascending=False)[:num_airports].index.tolist()\n",
    "    return airports\n",
    "    \n",
    "#function to restrict to list of airports\n",
    "#this helps limit training data size and in most cases gives consistent lists of airports for train, val, and test\n",
    "def airport_restrict(df,airports):\n",
    "    restricted=df[(df['Dest'].isin(airports))&(df['Origin'].isin(airports))]\n",
    "    return restricted\n",
    "\n",
    "\n",
    "#function to give airlines actual names\n",
    "#translates select airlines that have been integrated into other airlines over time\n",
    "def airline_names(df):\n",
    "    carriers=pd.read_csv('US_DOT/On-Time/L_UNIQUE_CARRIERS.csv')\n",
    "    carrier_dict=pd.Series(carriers['Description'].values,index=carriers['Code']).to_dict()\n",
    "    df_new=df.copy()\n",
    "    df_new['Reporting_Airline']=df['Reporting_Airline'].map(carrier_dict)\n",
    "    df_new.loc[df_new['Reporting_Airline']=='Virgin America','Reporting_Airline']='Alaska Airlines Inc.'  #Virgin America integrated into Alaska Airlines in 2018\n",
    "    return df_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to extract and preprocess model features and target variable from data that has been read from 'On-Time' csv files. Supports feature cleaning, transformation, and normalization, as well as selection of a random subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to encode cyclical time data into sin and cos variables\n",
    "#usful resource at https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning\n",
    "def convert_cyclical(df,col_name_maxval_dict):\n",
    "    df_copy=df.copy()\n",
    "    for col in col_name_maxval_dict.keys():\n",
    "        max_value=col_name_maxval_dict[col]\n",
    "        df_copy['sin('+col+')']=np.sin(2*np.pi*df_copy[col]/max_value)\n",
    "        df_copy['cos('+col+')']=np.cos(2*np.pi*df_copy[col]/max_value)\n",
    "        df_copy.drop(col,axis=1,inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "#function to convert times to decimal hours. \n",
    "#Format in file is an integer with hour in thousands and hundreds place and minutes in tens and ones place \n",
    "#(For example, in input csv 1415 corresponds to 2:15pm, convert to decimal hour 14.25)\n",
    "def convert_hours(df,columns):\n",
    "    df_copy=df.copy()\n",
    "    for col in columns:\n",
    "        df_copy[col]=df[col]//100+(df[col]%100)/60\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "#function to produce the desired predictor and target variable dataframes\n",
    "def predictors_and_target(df,X_vars,X_vars_categorical,target_delay_minutes=1):\n",
    "    subset=df[X_vars]\n",
    "    X=pd.get_dummies(subset,columns=X_vars_categorical)#one-hot encode the categorical variables\n",
    "    y=df['ArrDelayMinutes']>=target_delay_minutes\n",
    "    return X,y\n",
    "\n",
    "\n",
    "#function to choose random subset of matching predictor and target rows\n",
    "#if subset_frac is not less than 1, returns copies of original X and y\n",
    "def rand_subset(X,y,subset_frac,rand_state=0):\n",
    "    if subset_frac<1:\n",
    "        X_copy=X.copy()\n",
    "        X_copy['y']=y\n",
    "        samp=X_copy.sample(frac=subset_frac,random_state=rand_state)\n",
    "        y_samp=samp['y']\n",
    "        X_samp=samp.drop(['y'],axis=1)\n",
    "    else:\n",
    "        X_samp=X.copy()\n",
    "        y_samp=y.copy()\n",
    "    return X_samp,y_samp\n",
    "\n",
    "\n",
    "#function to choose imbalanced random subset of matching predictor and target rows\n",
    "#if subset_frac is not less than 1, returns copies of original X and y\n",
    "#if sum(y==True)>=len(y)*subset_frac//2, will return rows with equal number of T and F values in y\n",
    "#if sum(y==True)<len(y)*subset_frac//2, include all rows where y is T and len(y)*subset_frac//2 rows where y is F\n",
    "#same conditions on sum(y==False)\n",
    "def imbalanced_subsample(X,y,subset_frac,rand_state=0):\n",
    "    if subset_frac<1:\n",
    "        X_copy=X.copy()\n",
    "        X_copy['y']=y\n",
    "        X_copy_T=X_copy[X_copy['y']==True]\n",
    "        X_copy_F=X_copy[X_copy['y']==False]\n",
    "        num_samp=X_copy.shape[0]*subset_frac\n",
    "        if X_copy_T.shape[0]>=num_samp//2:\n",
    "            samp_T=X_copy_T.sample(n=int(num_samp//2),random_state=rand_state)\n",
    "        else:\n",
    "            samp_T=X_copy_T\n",
    "        if X_copy_F.shape[0]>=num_samp//2:\n",
    "            samp_F=X_copy_F.sample(n=int(num_samp//2),random_state=rand_state)\n",
    "        else:\n",
    "            samp_F=X_copy_F\n",
    "        samp=samp_T.append(samp_F)\n",
    "        y_samp=samp['y']\n",
    "        X_samp=samp.drop(['y'],axis=1)\n",
    "    else:\n",
    "        X_samp=X.copy()\n",
    "        y_samp=y.copy()\n",
    "    return X_samp,y_samp\n",
    "\n",
    "\n",
    "#function to normalize continuous varaibles in train and val data\n",
    "def normalize(X_train,X_val,X_vars_normalize,X_vars_log):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler=StandardScaler()\n",
    "    \n",
    "    train_to_scale=X_train.loc[:,X_vars_normalize]\n",
    "    val_to_scale=X_val.loc[:,X_vars_normalize]\n",
    "    train_to_log=train_to_scale.loc[:,X_vars_log]\n",
    "    val_to_log=val_to_scale.loc[:,X_vars_log]\n",
    "    \n",
    "    X_train_out=X_train.copy()\n",
    "    X_val_out=X_val.copy()\n",
    "    \n",
    "    train_to_scale[X_vars_log]=np.log(train_to_log)\n",
    "    train_scaled=scaler.fit_transform(train_to_scale)\n",
    "    X_train_out[X_vars_normalize]=train_scaled\n",
    "    \n",
    "    val_to_scale[X_vars_log]=np.log(val_to_log)\n",
    "    val_scaled=scaler.transform(val_to_scale)\n",
    "    X_val_out[X_vars_normalize]=val_scaled\n",
    "\n",
    "    \n",
    "    return X_train_out,X_val_out,scaler\n",
    "\n",
    "#function to add categorical variables from train that are missing in validation\n",
    "#(for example, an airport included in train that was not flown to in validation)\n",
    "#note however that this eliminates extra columns in the target dataframe (if you insert an airport or airline not in train, etc.)\n",
    "def missing_cat_add(X_train,X_val):\n",
    "    X_val_new=X_val.reindex(columns=X_train.columns.values.tolist(),fill_value=0)\n",
    "    \n",
    "    return X_val_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to support full sequence of importing data: reading csv files, processing and subsamping data, and extracting predictive features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read csv flight data, restrict airport list, convert hour and other cyclical data, and return predictor and target\n",
    "#Must give either the number of airports (for train) or an airport list (for val and test)\n",
    "def import_data(filenames,rand_frac,X_vars,X_vars_categorical,y_var,delay_minutes,\n",
    "                X_vars_hours,X_vars_cyclical_dict,\n",
    "                num_airports=None,airports_list=None,subsample_imbalanced=False):\n",
    "    \n",
    "    df=read_flight_csv(filenames,X_vars+y_var)\n",
    "    df.dropna(inplace=True)  \n",
    "    #EDA not shown here finds a few hundred CRSElapsedTime values missing some months, \n",
    "    #otherwise few tens of thousands of delay time targets missing out of half a million flights in a month\n",
    "    if airports_list is not None:\n",
    "        airports=airports_list\n",
    "    else:\n",
    "        airports=airport_select(df,num_airports)\n",
    "    df_restricted=airline_names(airport_restrict(df,airports))\n",
    "    \n",
    "    X,y_full=predictors_and_target(df_restricted,X_vars,X_vars_categorical,delay_minutes)\n",
    "    X_hours=convert_hours(X,X_vars_hours)\n",
    "    X_full=convert_cyclical(X_hours,X_vars_cyclical_dict)\n",
    "    \n",
    "    if subsample_imbalanced:\n",
    "        X_samp,y_samp=imbalanced_subsample(X_full,y_full,rand_frac)\n",
    "    else:\n",
    "        X_samp,y_samp=rand_subset(X_full,y_full,rand_frac)\n",
    "    \n",
    "    return X_samp,y_samp,airports,X_full,y_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Keras modeling procedure. Includes one dense layer and a dropout layer within a sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gtate\\Anaconda3\\envs\\tensorflow_environment\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#create modeling procedure\n",
    "#usful reference at https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "\n",
    "Metrics=[\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc')]\n",
    "\n",
    "def make_model(metrics=Metrics):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(\n",
    "            16, activation='relu',\n",
    "            input_shape=(X_train_normalized.shape[-1],)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid')])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads data, runs model training, and evaluates against the validation dataset. Prints loading, modeling, and evaluation progress to the terminal. Saves model, scaler, X column names, and X_vars_log. If train_new_model==False, loads existing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "\n",
      "start data import 2021-07-23 17:21:48.011512\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_5.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_6.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_7.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_8.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_9.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_10.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_11.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_12.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_1.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_2.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_3.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_4.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_5.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_6.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_7.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_8.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_9.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_10.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_11.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_12.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_1.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_2.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_3.csv\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_4.csv\n",
      "imported train 2021-07-23 17:23:12.972233\n",
      "Read US_DOT/On-Time/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_5.csv\n",
      "imported validation 2021-07-23 17:23:16.259285\n",
      "data normalized 2021-07-23 17:23:18.043106\n",
      "modeling begin 2021-07-23 17:23:18.420809\n",
      "Train on 3100719 samples, validate on 394341 samples\n",
      "Epoch 1/15\n",
      "3100719/3100719 [==============================] - 56s 18us/sample - loss: 0.4336 - tp: 5964.0000 - fp: 38725.0000 - tn: 2622756.0000 - fn: 433274.0000 - accuracy: 0.8478 - precision: 0.1335 - recall: 0.0136 - auc: 0.5664 - val_loss: 0.3877 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 341527.0000 - val_fn: 52814.0000 - val_accuracy: 0.8661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6166\n",
      "Epoch 2/15\n",
      "3100719/3100719 [==============================] - 57s 18us/sample - loss: 0.4040 - tp: 355.0000 - fp: 374.0000 - tn: 2661107.0000 - fn: 438883.0000 - accuracy: 0.8583 - precision: 0.4870 - recall: 8.0822e-04 - auc: 0.6020 - val_loss: 0.3868 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 341527.0000 - val_fn: 52814.0000 - val_accuracy: 0.8661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6154\n",
      "Epoch 3/15\n",
      "3100719/3100719 [==============================] - 56s 18us/sample - loss: 0.3958 - tp: 1805.0000 - fp: 2042.0000 - tn: 2659439.0000 - fn: 437433.0000 - accuracy: 0.8583 - precision: 0.4692 - recall: 0.0041 - auc: 0.6254 - val_loss: 0.3850 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 341527.0000 - val_fn: 52814.0000 - val_accuracy: 0.8661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6205\n",
      "Epoch 4/15\n",
      "3100719/3100719 [==============================] - 58s 19us/sample - loss: 0.3936 - tp: 2791.0000 - fp: 3432.0000 - tn: 2658049.0000 - fn: 436447.0000 - accuracy: 0.8581 - precision: 0.4485 - recall: 0.0064 - auc: 0.6358 - val_loss: 0.3857 - val_tp: 9.0000 - val_fp: 18.0000 - val_tn: 341509.0000 - val_fn: 52805.0000 - val_accuracy: 0.8660 - val_precision: 0.3333 - val_recall: 1.7041e-04 - val_auc: 0.6185\n",
      "Epoch 5/15\n",
      "3100719/3100719 [==============================] - 58s 19us/sample - loss: 0.3931 - tp: 3112.0000 - fp: 3905.0000 - tn: 2657576.0000 - fn: 436126.0000 - accuracy: 0.8581 - precision: 0.4435 - recall: 0.0071 - auc: 0.6384 - val_loss: 0.3848 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 341527.0000 - val_fn: 52814.0000 - val_accuracy: 0.8661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6208\n",
      "Epoch 6/15\n",
      "3100719/3100719 [==============================] - 57s 18us/sample - loss: 0.3929 - tp: 3376.0000 - fp: 4126.0000 - tn: 2657355.0000 - fn: 435862.0000 - accuracy: 0.8581 - precision: 0.4500 - recall: 0.0077 - auc: 0.6396 - val_loss: 0.3846 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 341527.0000 - val_fn: 52814.0000 - val_accuracy: 0.8661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6209\n",
      "Epoch 7/15\n",
      "3100719/3100719 [==============================] - 57s 18us/sample - loss: 0.3927 - tp: 3368.0000 - fp: 4339.0000 - tn: 2657142.0000 - fn: 435870.0000 - accuracy: 0.8580 - precision: 0.4370 - recall: 0.0077 - auc: 0.6404 - val_loss: 0.3851 - val_tp: 4.0000 - val_fp: 8.0000 - val_tn: 341519.0000 - val_fn: 52810.0000 - val_accuracy: 0.8661 - val_precision: 0.3333 - val_recall: 7.5737e-05 - val_auc: 0.6202\n",
      "Epoch 8/15\n",
      "3100719/3100719 [==============================] - 56s 18us/sample - loss: 0.3925 - tp: 3392.0000 - fp: 4289.0000 - tn: 2657192.0000 - fn: 435846.0000 - accuracy: 0.8581 - precision: 0.4416 - recall: 0.0077 - auc: 0.6415 - val_loss: 0.3856 - val_tp: 7.0000 - val_fp: 16.0000 - val_tn: 341511.0000 - val_fn: 52807.0000 - val_accuracy: 0.8660 - val_precision: 0.3043 - val_recall: 1.3254e-04 - val_auc: 0.6217\n",
      "Epoch 9/15\n",
      "3100719/3100719 [==============================] - 55s 18us/sample - loss: 0.3924 - tp: 3512.0000 - fp: 4250.0000 - tn: 2657231.0000 - fn: 435726.0000 - accuracy: 0.8581 - precision: 0.4525 - recall: 0.0080 - auc: 0.6419 - val_loss: 0.3853 - val_tp: 4.0000 - val_fp: 4.0000 - val_tn: 341523.0000 - val_fn: 52810.0000 - val_accuracy: 0.8661 - val_precision: 0.5000 - val_recall: 7.5737e-05 - val_auc: 0.6201\n",
      "Epoch 10/15\n",
      "3100719/3100719 [==============================] - 58s 19us/sample - loss: 0.3923 - tp: 3578.0000 - fp: 4351.0000 - tn: 2657130.0000 - fn: 435660.0000 - accuracy: 0.8581 - precision: 0.4513 - recall: 0.0081 - auc: 0.6425 - val_loss: 0.3852 - val_tp: 2.0000 - val_fp: 8.0000 - val_tn: 341519.0000 - val_fn: 52812.0000 - val_accuracy: 0.8661 - val_precision: 0.2000 - val_recall: 3.7869e-05 - val_auc: 0.6219\n",
      "Epoch 11/15\n",
      "3100719/3100719 [==============================] - 59s 19us/sample - loss: 0.3922 - tp: 3622.0000 - fp: 4408.0000 - tn: 2657073.0000 - fn: 435616.0000 - accuracy: 0.8581 - precision: 0.4511 - recall: 0.0082 - auc: 0.6428 - val_loss: 0.3846 - val_tp: 0.0000e+00 - val_fp: 4.0000 - val_tn: 341523.0000 - val_fn: 52814.0000 - val_accuracy: 0.8661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6200\n",
      "Epoch 12/15\n",
      "3100719/3100719 [==============================] - 55s 18us/sample - loss: 0.3921 - tp: 3557.0000 - fp: 4316.0000 - tn: 2657165.0000 - fn: 435681.0000 - accuracy: 0.8581 - precision: 0.4518 - recall: 0.0081 - auc: 0.6435 - val_loss: 0.3848 - val_tp: 0.0000e+00 - val_fp: 4.0000 - val_tn: 341523.0000 - val_fn: 52814.0000 - val_accuracy: 0.8661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6201\n",
      "Epoch 13/15\n",
      "3100719/3100719 [==============================] - 55s 18us/sample - loss: 0.3920 - tp: 3545.0000 - fp: 4358.0000 - tn: 2657123.0000 - fn: 435693.0000 - accuracy: 0.8581 - precision: 0.4486 - recall: 0.0081 - auc: 0.6437 - val_loss: 0.3850 - val_tp: 1.0000 - val_fp: 7.0000 - val_tn: 341520.0000 - val_fn: 52813.0000 - val_accuracy: 0.8661 - val_precision: 0.1250 - val_recall: 1.8934e-05 - val_auc: 0.6218\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3100719/3100719 [==============================] - 57s 18us/sample - loss: 0.3919 - tp: 3469.0000 - fp: 4332.0000 - tn: 2657149.0000 - fn: 435769.0000 - accuracy: 0.8581 - precision: 0.4447 - recall: 0.0079 - auc: 0.6440 - val_loss: 0.3856 - val_tp: 0.0000e+00 - val_fp: 4.0000 - val_tn: 341523.0000 - val_fn: 52814.0000 - val_accuracy: 0.8661 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6202\n",
      "Epoch 15/15\n",
      "3100719/3100719 [==============================] - 54s 17us/sample - loss: 0.3919 - tp: 3457.0000 - fp: 4333.0000 - tn: 2657148.0000 - fn: 435781.0000 - accuracy: 0.8581 - precision: 0.4438 - recall: 0.0079 - auc: 0.6444 - val_loss: 0.3844 - val_tp: 1.0000 - val_fp: 7.0000 - val_tn: 341520.0000 - val_fn: 52813.0000 - val_accuracy: 0.8661 - val_precision: 0.1250 - val_recall: 1.8934e-05 - val_auc: 0.6230\n",
      "model fitted2021-07-23 17:37:32.734292\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set train and val files\n",
    "train_files=make_filenames(train_list_start,train_list_end)\n",
    "val_files=make_filenames(val_list_start,val_list_end)\n",
    "\n",
    "train_range=train_list_start+'-'+train_list_end\n",
    "\n",
    " \n",
    "print('******************************\\n')\n",
    "\n",
    "    \n",
    "##############################\n",
    "#import and normalize data\n",
    "print('start data import '+str(datetime.now()))\n",
    "X_train,y_train,airports,_,y_train_full=import_data(train_files,rand_frac_train,X_vars,X_vars_categorical,y_var,delay_minutes,\n",
    "                                X_vars_hours,X_vars_cyclical_dict,num_airports=num_airports,\n",
    "                                subsample_imbalanced=imbalanced)\n",
    "print('imported train '+str(datetime.now()))\n",
    "\n",
    "\n",
    "X_val,y_val,_,_,_=import_data(val_files,rand_frac_val,X_vars,X_vars_categorical,y_var,delay_minutes,\n",
    "                                X_vars_hours,X_vars_cyclical_dict,airports_list=airports,subsample_imbalanced=imbalanced)\n",
    "print('imported validation '+str(datetime.now()))\n",
    "\n",
    "\n",
    "X_train_normalized,X_val_norm0,scaler=normalize(X_train,X_val,X_vars_normalize,X_vars_log)\n",
    "X_val_normalized=missing_cat_add(X_train_normalized,X_val_norm0)\n",
    "print('data normalized '+str(datetime.now()))\n",
    "    \n",
    "\n",
    "##############################\n",
    "#train model\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc',verbose=1,patience=10,\n",
    "                                                    mode='max',restore_best_weights=True)\n",
    "model=make_model()\n",
    "\n",
    "print('modeling begin '+str(datetime.now()))\n",
    "history=model.fit(X_train_normalized,y_train,batch_size=Batch_Size,epochs=Epochs,\n",
    "                    validation_data=(X_val_normalized,y_val))\n",
    "print('model fitted'+str(datetime.now()))\n",
    "\n",
    "\n",
    "print('******************************\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and other variables needed to later make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 7-22_May2019-Apr2021\\saved_model\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7-22_May2019-Apr2021/subsample_imbalanced.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save model\n",
    "model.save(output_folder.split('/')[0]+'\\saved_model')\n",
    "\n",
    "#save scaler\n",
    "from joblib import dump\n",
    "dump(scaler,output_folder+'scaler.joblib')\n",
    "\n",
    "#save distribution of predicted probabilities for validation data\n",
    "val_probs=model.predict_proba(X_val_normalized)\n",
    "dump(val_probs,output_folder+'val_probs.joblib')\n",
    "\n",
    "#save training data characteristics\n",
    "dump(X_train.columns.values.tolist(),output_folder+'X_train_cols.joblib')\n",
    "dump(np.mean(y_train_full),output_folder+'Train_delay_proportion.joblib')\n",
    "dump(airports,output_folder+'airports.joblib')\n",
    "\n",
    "#save processing inputs\n",
    "dump(X_vars_normalize,output_folder+'X_vars_normalize.joblib')\n",
    "dump(X_vars_log,output_folder+'X_vars_log.joblib')\n",
    "dump(X_vars_categorical,output_folder+'X_vars_categorical.joblib')\n",
    "dump(X_vars_hours,output_folder+'X_vars_hours.joblib')\n",
    "dump(X_vars_cyclical_dict,output_folder+'X_vars_cyclical_dict.joblib')\n",
    "dump(delay_minutes,output_folder+'delay_minutes.joblib')\n",
    "dump(imbalanced,output_folder+'subsample_imbalanced.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful tensorflow resources: <br>\n",
    "https://www.tensorflow.org/tutorials/structured_data/feature_columns, <br>\n",
    "https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers, <br>\n",
    "https://www.tensorflow.org/tutorials/structured_data/time_series,  <br>\n",
    "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data <br>\n",
    "\n",
    "Cyclical time discussion: https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
